parameters:
- name: environmentName
  type: string
- name: dbclusterid
  type: string

jobs:
- deployment: deploy_databricks
  displayName: 'Deploy to Databricks'
  pool:
    vmImage: 'ubuntu-latest'
  variables:
    pythonVersion: 3.8
  environment: ${{ parameters.environmentName }}
  strategy:
    runOnce:
      deploy:
        steps:     
        - task: UsePythonVersion@0
          inputs:
            versionSpec: '$(pythonVersion)'
            addToPath: true
            architecture: 'x64'
          displayName: 'Use Python Version: $(pythonVersion)'

        - script: |
            packageWheelName=$(ls ${WHEEL_FILE_PATH} | grep '^ddo_transform.*.whl$' | head -n 1)
            echo "Package wheel name is: $packageWheelName"
            echo "##vso[task.setvariable variable=packageWheelName;isOutput=true]$packageWheelName"
          env:
            WHEEL_FILE_PATH: $(Pipeline.Workspace)/ciartifacts/dist
          name: setPackageWheelName
          displayName: 'Set packageWheelName variable'

        - script: |
            wget https://github.com/databricks/cli/releases/download/v0.233.0/databricks_cli_0.233.0_linux_amd64.zip -O databricks_cli.zip
            unzip databricks_cli.zip -d /usr/local/bin  
            echo "Installed $("databricks" -v)"
            echo "Uploading app libraries to DBFS..."
            echo "Uploading SOURCE whl file: ${WHEEL_FILE_PATH}/${WHEEL_FILE_NAME} to dbfs DESTINATION: ${DBFS_LIBS_PATH}/${WHEEL_FILE_NAME}"
            databricks fs cp --recursive --overwrite "${WHEEL_FILE_PATH}/${WHEEL_FILE_NAME}" "${DBFS_LIBS_PATH}/${WHEEL_FILE_NAME}"
            echo "Uploading notebooks at ${NOTEBOOKS_PATH} to workspace (${DATABRICKS_NOTEBOOK_PATH})..."
            databricks workspace import_dir --recursive --overwrite "${NOTEBOOKS_PATH}" "${DATABRICKS_NOTEBOOK_PATH}"
            # Create JSON file for library installation
            json_file="./databricks/config/libs.config.json"
            cat <<EOF > $json_file
            {
              "cluster_id": ${{ parameters.dbclusterid }},
              "libraries": [
                {
                  "whl": "dbfs:/ddo_transform-localdev-py2.py3-none-any.whl"
                }
              ]
            }
            EOF
            
            # Install library on the cluster using the JSON file
            databricks libraries install --json @$json_file
          env:
            DATABRICKS_HOST: $(databricksDomain)
            DATABRICKS_TOKEN: $(databricksToken)
            DBFS_LIBS_PATH: $(databricksDbfsLibPath)
            WHEEL_FILE_PATH: $(Pipeline.Workspace)/ciartifacts/dist
            WHEEL_FILE_NAME: $(setPackageWheelName.packageWheelName)
            NOTEBOOKS_PATH: $(Pipeline.Workspace)/ciartifacts/databricks/notebooks
            DATABRICKS_NOTEBOOK_PATH: $(databricksNotebookPath)
          displayName: 'Install Databricks CLI and Deploy notebooks and packages'
